"""
LLM (Large Language Model) ì„œë¹„ìŠ¤

ì´ ëª¨ë“ˆì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ì£¼ì–´ì§„ í…ìŠ¤íŠ¸(í”„ë¡¬í”„íŠ¸)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
ìì—°ì–´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import time
from typing import Optional, Tuple
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from app.settings import settings
from app.utils.logging import get_logger

logger = get_logger("llm")

# --- ì „ì—­ ëª¨ë¸ ìºì‹œ ---
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì „ì—­ ë³€ìˆ˜ì…ë‹ˆë‹¤.
# ì´ë¥¼ í†µí•´ ë§¤ë²ˆ API ìš”ì²­ ì‹œë§ˆë‹¤ ëª¨ë¸ì„ ìƒˆë¡œ ë¡œë“œí•˜ëŠ” ì˜¤ë²„í—¤ë“œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
_tokenizer: Optional[AutoTokenizer] = None
_llm_model: Optional[AutoModelForCausalLM] = None


def get_llm_models(model_id: Optional[str] = None) -> Tuple[Optional[AutoTokenizer], Optional[AutoModelForCausalLM]]:
    """
    LLM ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì—° ë¡œë“œ(lazy loading)í•˜ê³  ì „ì—­ì ìœ¼ë¡œ ìºì‹±í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œê°€ ì•„ë‹Œ, ì‹¤ì œ LLM ê¸°ëŠ¥ì´ ì²˜ìŒ í•„ìš”í•  ë•Œ ëª¨ë¸ì„
    ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì´ˆê¸° êµ¬ë™ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    í•œ ë²ˆ ë¡œë“œëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” ì „ì—­ ë³€ìˆ˜ì— ìºì‹œë˜ì–´ ì´í›„ ìš”ì²­ì—ì„œëŠ” ì¦‰ì‹œ ë°˜í™˜ë©ë‹ˆë‹¤.

    - `settings.LLM_ENABLED`ê°€ `False`ì´ë©´ ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•Šê³  `(None, None)`ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - `device_map="auto"` ì„¤ì •ì„ í†µí•´ ê°€ëŠ¥í•œ ê²½ìš° GPU(CUDA)ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°,
      GPUê°€ ì—†ìœ¼ë©´ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - Gemma ê³„ì—´ ëª¨ë¸ì˜ ê²½ìš° `torch.bfloat16` ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•©ë‹ˆë‹¤.

    Args:
        model_id (str, optional): ë¡œë“œí•  ëª¨ë¸ì˜ Hugging Face ID.
                                    ì œê³µë˜ì§€ ì•Šìœ¼ë©´ `settings.LLM_MODEL`ì˜ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Returns:
        Tuple[Optional[AutoTokenizer], Optional[AutoModelForCausalLM]]:
            ì„±ê³µ ì‹œ (í† í¬ë‚˜ì´ì €, ëª¨ë¸) íŠœí”Œ, ì‹¤íŒ¨ ë˜ëŠ” ë¹„í™œì„±í™” ì‹œ (None, None)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global _tokenizer, _llm_model
    
    if not settings.LLM_ENABLED:
        logger.info("LLM ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None, None
    
    # ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ë•Œë§Œ ì´ˆê¸°í™” ìˆ˜í–‰
    if _llm_model is None:
        model_id = model_id or settings.LLM_MODEL
        logger.info(f"ğŸ”„ LLM ëª¨ë¸('{model_id}')ì˜ ì§€ì—° ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        try:
            # Gemma ê³„ì—´ ëª¨ë¸ (medgemma, gemma-2 í¬í•¨)ì„ ìœ„í•œ ì„¤ì •
            if "gemma" in model_id:
                logger.info(f"Gemma ê³„ì—´ ëª¨ë¸ ('{model_id}')ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
                _tokenizer = AutoTokenizer.from_pretrained(model_id)
                _llm_model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                )
            else:
                # ì¼ë°˜ ëª¨ë¸ ë¡œë“œ
                logger.info(f"ì¼ë°˜ LLM ëª¨ë¸ ('{model_id}')ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
                _tokenizer = AutoTokenizer.from_pretrained(model_id)
                _llm_model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    device_map="auto",
                )
            
            logger.info(f"âœ… LLM ëª¨ë¸('{model_id}')ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ LLM ëª¨ë¸('{model_id}') ë¡œë”© ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", exc_info=True)
            _tokenizer = None
            _llm_model = None
    
    return _tokenizer, _llm_model


def generate_response(
    prompt: str,
    max_new_tokens: int = 512,
    temperature: float = 0.7,
    repetition_penalty: float = 1.2
) -> str:
    """
    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        prompt (str): ëª¨ë¸ì— ì…ë ¥ë  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸.
        max_new_tokens (int): ìƒì„±ë  í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ (í† í° ê¸°ì¤€).
        temperature (float): ìƒì„±ì˜ ë¬´ì‘ìœ„ì„±ì„ ì¡°ì ˆí•˜ëŠ” ê°’. ë‚®ì„ìˆ˜ë¡ ê²°ì •ë¡ ì , ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±ì´ ì¦ê°€í•©ë‹ˆë‹¤.
        repetition_penalty (float): ì‘ë‹µì—ì„œ ë‹¨ì–´ ë°˜ë³µì„ ì–µì œí•˜ëŠ” ê°•ë„. 1.0 ì´ìƒìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

    Returns:
        str: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸. ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì²´ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    
    try:
        tokenizer, model = get_llm_models()
        
        if tokenizer is None or model is None:
            logger.warning("LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì–¸ì–´ ëª¨ë¸ ì„œë¹„ìŠ¤ë¥¼ í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # MedGemma ëª¨ë¸ì€ ì±„íŒ… í…œí”Œë¦¿ ì‚¬ìš©
        if "medgemma" in model.name_or_path:
            messages = [
                {"role": "system", "content": "You are a helpful medical assistant."},
                {"role": "user", "content": prompt}
            ]
            inputs = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_tensors="pt",
            ).to(model.device)
        else:
            # ê¸°ì¡´ ëª¨ë¸ ì…ë ¥ ë°©ì‹
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        input_len = inputs["input_ids"].shape[-1]

        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”í•˜ì—¬ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.eos_token_id,
                temperature=temperature,
                repetition_penalty=repetition_penalty,
                do_sample=True,  # temperature, top_p, top_k ë“±ì˜ ì˜µì…˜ì„ í™œì„±í™”í•˜ë ¤ë©´ Trueë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
                top_k=50,
                top_p=0.95,
            )

        # ìƒì„±ëœ ê²°ê³¼ì—ì„œ ì…ë ¥ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ìˆœìˆ˜í•œ ì‘ë‹µë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)

        processing_time = (time.time() - start_time) * 1000
        logger.info(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ ({processing_time:.2f}ms)")

        return response.strip()
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"âŒ LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({processing_time:.2f}ms): {e}", exc_info=True)
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
