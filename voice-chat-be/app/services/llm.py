"""
LLM (Large Language Model) ì„œë¹„ìŠ¤
"""

import time
from typing import Optional, Tuple
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from app.settings import settings
from app.utils.logging import get_logger

logger = get_logger("llm")

# --- ì „ì—­ ëª¨ë¸ ìºì‹œ ---
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì „ì—­ ë³€ìˆ˜ì…ë‹ˆë‹¤.
# ì´ë¥¼ í†µí•´ ë§¤ë²ˆ API ìš”ì²­ ì‹œë§ˆë‹¤ ëª¨ë¸ì„ ìƒˆë¡œ ë¡œë“œí•˜ëŠ” ì˜¤ë²„í—¤ë“œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
_tokenizer: Optional[AutoTokenizer] = None
_llm_model: Optional[AutoModelForCausalLM] = None


def get_llm_models(model_id: Optional[str] = None) -> Tuple[Optional[AutoTokenizer], Optional[AutoModelForCausalLM]]:
    """
    LLM ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì—° ë¡œë“œ(lazy loading)í•©ë‹ˆë‹¤.
    - `LLM_ENABLED` ì„¤ì •ì´ `False`ì´ë©´ ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°(`_llm_model` is None), Hugging Face Hubì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ 
      ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì—¬ ì „ì—­ ë³€ìˆ˜ `_tokenizer`ì™€ `_llm_model`ì— ìºì‹œí•©ë‹ˆë‹¤.
    - ì´ë¯¸ ë¡œë“œëœ ê²½ìš°, ìºì‹œëœ ê°ì²´ë¥¼ ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤.
    - ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° CUDA (GPU)ë¥¼ ì‚¬ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        model_id (str, optional): ë¡œë“œí•  ëª¨ë¸ì˜ Hugging Face ID. ê¸°ë³¸ê°’ì€ ì„¤ì • íŒŒì¼ì— ë”°ë¦…ë‹ˆë‹¤.
        
    Returns:
        Tuple[Optional[AutoTokenizer], Optional[AutoModelForCausalLM]]: ë¡œë“œëœ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ê°ì²´.
                                                                       ì‹¤íŒ¨ ì‹œ (None, None).
    """
    global _tokenizer, _llm_model
    
    if not settings.LLM_ENABLED:
        logger.info("LLM ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None, None
    
    # ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ë•Œë§Œ ì´ˆê¸°í™” ìˆ˜í–‰
    if _llm_model is None:
        model_id = model_id or settings.LLM_MODEL
        logger.info(f"ğŸ”„ LLM ëª¨ë¸('{model_id}')ì˜ ì§€ì—° ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        try:
            # Gemma ê³„ì—´ ëª¨ë¸ (medgemma, gemma-2 í¬í•¨)ì„ ìœ„í•œ ì„¤ì •
            if "gemma" in model_id:
                logger.info(f"Gemma ê³„ì—´ ëª¨ë¸ ('{model_id}')ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
                _tokenizer = AutoTokenizer.from_pretrained(model_id)
                _llm_model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                )
            else:
                # ì¼ë°˜ ëª¨ë¸ ë¡œë“œ
                logger.info(f"ì¼ë°˜ LLM ëª¨ë¸ ('{model_id}')ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
                _tokenizer = AutoTokenizer.from_pretrained(model_id)
                _llm_model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    device_map="auto",
                )
            
            logger.info(f"âœ… LLM ëª¨ë¸('{model_id}')ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ LLM ëª¨ë¸('{model_id}') ë¡œë”© ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", exc_info=True)
            _tokenizer = None
            _llm_model = None
    
    return _tokenizer, _llm_model


def generate_response(
    prompt: str,
    max_length: int = 512,
    temperature: float = 0.7
) -> str:
    """
    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        prompt (str): ëª¨ë¸ì— ì…ë ¥ë  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸.
        max_length (int): ìƒì„±ë  í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ (í† í° ê¸°ì¤€).
        temperature (float): ìƒì„±ì˜ ë¬´ì‘ìœ„ì„±ì„ ì¡°ì ˆí•˜ëŠ” ê°’. ë‚®ì„ìˆ˜ë¡ ê²°ì •ë¡ ì , ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±ì´ ì¦ê°€í•©ë‹ˆë‹¤.
    
    Returns:
        str: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸. ì˜¤ë¥˜ ë°œìƒ ì‹œì—ëŠ” ëŒ€ì²´ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    
    try:
        tokenizer, model = get_llm_models()
        
        if tokenizer is None or model is None:
            logger.warning("LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì–¸ì–´ ëª¨ë¸ ì„œë¹„ìŠ¤ë¥¼ í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # MedGemma ëª¨ë¸ì€ ì±„íŒ… í…œí”Œë¦¿ ì‚¬ìš©
        if "medgemma" in model.name_or_path:
            messages = [
                {"role": "system", "content": "You are a helpful medical assistant."},
                {"role": "user", "content": prompt}
            ]
            inputs = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_tensors="pt",
            ).to(model.device)
        else:
            # ê¸°ì¡´ ëª¨ë¸ ì…ë ¥ ë°©ì‹
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        input_len = inputs["input_ids"].shape[-1]

        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”í•˜ì—¬ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,      # ì‘ë‹µì˜ ìµœëŒ€ ê¸¸ì´ ì œí•œ (max_length -> max_new_tokens)
                pad_token_id=tokenizer.eos_token_id  # íŒ¨ë”© í† í°ì„ ë¬¸ì¥ ë(EOS) í† í°ìœ¼ë¡œ ì„¤ì •
            )
        
        # ìƒì„±ëœ í† í° IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤.
        # ì¶œë ¥ í…ì„œì˜ ì²« ë²ˆì§¸ í•­ëª© ì „ì²´ë¥¼ ë””ì½”ë”©í•©ë‹ˆë‹¤.
        # skip_special_tokens=True ì˜µì…˜ì´ íŠ¹ìˆ˜ í† í°(íŒ¨ë”©, ë¬¸ì¥ ì‹œì‘/ë ë“±)ì„ ì œê±°í•´ì¤ë‹ˆë‹¤.
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Gemma ëª¨ë¸ì˜ ê²½ìš°, ì‘ë‹µì— ì…ë ¥ í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.
        if not "medgemma" in model.name_or_path:
             if prompt in response:
                response = response.replace(prompt, "").strip()

        processing_time = (time.time() - start_time) * 1000
        logger.info(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ ({processing_time:.2f}ms)")
        
        return response
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"âŒ LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({processing_time:.2f}ms): {e}", exc_info=True)
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
